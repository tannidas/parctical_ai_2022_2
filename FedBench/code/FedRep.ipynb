{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ce8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b177919",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fbbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20 \n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 300\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()\n",
    "\n",
    "#args.rep_epochs = args.local_epochs//2\n",
    "#args.head_epochs = args.local_epochs - args.rep_epochs\n",
    "\n",
    "args.rep_epochs = args.local_epochs\n",
    "args.head_epochs = args.local_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3867cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859fa59f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(args, unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043a314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.global_model = eval(model_name)\n",
    "    self.global_model_key = eval(args.global_model_key)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.global_model = copy.deepcopy(self.global_model)\n",
    "      worker.global_model = worker.global_model.to(args.device)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.global_model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "      worker.global_model = worker.global_model.to('cpu')\n",
    "      del worker.global_model\n",
    "    self.global_model.load_state_dict(new_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.global_model = None\n",
    "    self.local_model = eval(model_name)\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    self.aggregation_weight = None\n",
    "\n",
    "  def local_train(self):\n",
    "    for name, param in self.local_model.named_parameters():\n",
    "        if name in server.global_model_key:\n",
    "            param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "    acc_train,loss_train = train(self.local_model,args.criterion,self.trainloader,args.head_epochs)\n",
    "    for name, param in self.local_model.named_parameters():\n",
    "        if name in server.global_model_key:\n",
    "            param.requires_grad = True\n",
    "        else:\n",
    "            param.requires_grad = False    \n",
    "    acc_train,loss_train = train(self.local_model,args.criterion,self.trainloader,args.rep_epochs)   \n",
    "    acc_valid,loss_valid = test(self.local_model,args.criterion,self.valloader)\n",
    "    self.global_model = copy.deepcopy(self.local_model)\n",
    "    return acc_train,loss_train,acc_valid,loss_valid\n",
    "\n",
    "  def set_model(self):\n",
    "    new_state = copy.deepcopy(self.global_model.state_dict())\n",
    "    local_state = self.local_model.state_dict()\n",
    "    for k in new_state.keys():\n",
    "        if k not in server.global_model_key:\n",
    "            new_state[k] = local_state[k]\n",
    "    self.local_model.load_state_dict(new_state)\n",
    "    self.local_model.to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.train()\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n",
    "\n",
    "  else:\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      model.train()\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95202505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "\n",
    "\n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad37a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_model(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    worker.set_model()\n",
    "    acc_train_tmp,loss_train_tmp,acc_valid_tmp,loss_valid_tmp = worker.local_train()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a95f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train time：{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ebf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test = []\n",
    "loss_test = []\n",
    "\n",
    "nums = 0\n",
    "for worker in workers:\n",
    "  nums += worker.test_data_num\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.aggregation_weight = 1.0*worker.test_data_num/nums\n",
    "  worker.local_model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(worker.local_model,args.criterion,worker.testloader)\n",
    "  acc_test.append(acc_tmp)\n",
    "  loss_test.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.local_model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_avg = sum(acc_test)/len(acc_test)\n",
    "loss_test_avg = sum(loss_test)/len(loss_test)\n",
    "print('Test  loss:{}  accuracy:{}'.format(loss_test_avg,acc_test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'FedRep_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831b8992",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = pd.DataFrame(acc_train)\n",
    "loss_train = pd.DataFrame(loss_train)\n",
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test = pd.DataFrame(acc_test)\n",
    "loss_test = pd.DataFrame(loss_test)\n",
    "\n",
    "\n",
    "acc_train.to_csv(result_path+filename+'_train_acc.csv',index=False, header=False)\n",
    "loss_train.to_csv(result_path+filename+'_train_loss.csv',index=False, header=False)\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test.to_csv(result_path+filename+'_test_acc.csv',index=False, header=False)\n",
    "loss_test.to_csv(result_path+filename+'_test_loss.csv',index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
