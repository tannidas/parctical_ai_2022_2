{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "vkZxat4Y-IsQ",
    "outputId": "da86392c-66e8-4b60-b471-086e745cdcbc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from torch import nn, optim\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import json\n",
    "import sys\n",
    "\n",
    "sys.path.append('./utils/')\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you want to experiment with a different seed value, change 'trial_times'\n",
    "trial_times = 1\n",
    "SEED = 42 + trial_times -1\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O0TfzOhU-QlG"
   },
   "outputs": [],
   "source": [
    "class Argments():\n",
    "  def __init__(self):\n",
    "    self.batch_size = 20 \n",
    "    self.test_batch = 1000\n",
    "    self.global_epochs = 300\n",
    "    self.local_epochs = 2\n",
    "    self.lamda = 15\n",
    "    self.K = 5\n",
    "    self.lr = 10**(-3)\n",
    "    self.momentum = 0.9\n",
    "    self.weight_decay = 10**-4.0\n",
    "    self.clip = 20.0\n",
    "    self.partience = 300\n",
    "    self.worker_num = 20\n",
    "    self.participation_rate = 1\n",
    "    self.sample_num = int(self.worker_num * self.participation_rate)\n",
    "    self.total_data_rate = 1\n",
    "    self.unlabeleddata_size = 1000\n",
    "    self.device = device = torch.device('cuda:0'if torch.cuda.is_available() else'cpu')\n",
    "    self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    ## If you use MNIST or CIFAR-10, the degree of data heterogeneity can be changed by changing alpha_label and alpha_size.\n",
    "    self.alpha_label = 0.5\n",
    "    self.alpha_size = 10\n",
    "    \n",
    "    ## Select a dataset from 'FEMNIST','Shakespeare','Sent140','MNIST', or 'CIFAR-10'.\n",
    "    self.dataset_name = 'FEMNIST'\n",
    "\n",
    "\n",
    "args = Argments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.dataset_name=='FEMNIST':\n",
    "    from femnist_dataset import *\n",
    "    args.num_classes = 62\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='Shakespeare':\n",
    "    from shakespeare_dataset import *\n",
    "    model_name = \"RNN()\"\n",
    "    \n",
    "elif args.dataset_name=='Sent140':\n",
    "    from sent140_dataset import *\n",
    "    from utils_sent140 import *\n",
    "    VOCAB_DIR = '../models/embs.json'\n",
    "    _, indd, vocab = get_word_emb_arr(VOCAB_DIR)\n",
    "    model_name = \"RNNSent(args,'LSTM', 2, 25, 128, 1, 0.5, tie_weights=False)\"\n",
    "    \n",
    "elif args.dataset_name=='MNIST':\n",
    "    from mnist_dataset import *\n",
    "    args.num_classes = 10\n",
    "    model_name = \"CNN(num_classes=args.num_classes)\"\n",
    "    \n",
    "elif args.dataset_name=='CIFAR-10':\n",
    "    from cifar10_dataset import *\n",
    "    model_name = \"vgg13()\"\n",
    "    \n",
    "else:\n",
    "    print('Error: The name of the dataset is incorrect. Please re-set the \"dataset_name\".')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_trainset,federated_valset,federated_testset,unlabeled_dataset = get_dataset(args, unlabeled_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pFedMeOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01, lamda=0.1 , mu = 0.001):\n",
    "        #self.local_weight_updated = local_weight # w_i,K\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        defaults = dict(lr=lr, lamda=lamda, mu = mu)\n",
    "        super(pFedMeOptimizer, self).__init__(params, defaults)\n",
    "    \n",
    "    def step(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        weight_update = local_weight_updated.copy()\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], weight_update):\n",
    "                localweight.data = localweight.data.to(args.device)\n",
    "                p.data = p.data - group['lr'] * (p.grad.data + group['lamda'] * (p.data - localweight.data) + group['mu']*p.data)\n",
    "                localweight.data = localweight.data.to('cpu')\n",
    "        return  group['params'], loss\n",
    "    \n",
    "    def update_param(self, local_weight_updated, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure\n",
    "        weight_update = local_weight_updated.copy()\n",
    "        for group in self.param_groups:\n",
    "            for p, localweight in zip( group['params'], weight_update):\n",
    "                p.data = localweight.data\n",
    "        #return  p.data\n",
    "        return  group['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yu90X1TWJVKJ"
   },
   "outputs": [],
   "source": [
    "class Server():\n",
    "  def __init__(self):\n",
    "    self.model = eval(model_name)\n",
    "\n",
    "  def create_worker(self,federated_trainset,federated_valset,federated_testset):\n",
    "    workers = []\n",
    "    for i in range(args.worker_num):\n",
    "      workers.append(Worker(federated_trainset[i],federated_valset[i],federated_testset[i]))\n",
    "    return workers\n",
    "\n",
    "  def sample_worker(self,workers):\n",
    "    sample_worker = []\n",
    "    sample_worker_num = random.sample(range(args.worker_num),args.sample_num)\n",
    "    for i in sample_worker_num:\n",
    "      sample_worker.append(workers[i])\n",
    "    return sample_worker\n",
    "\n",
    "\n",
    "  def send_model(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "\n",
    "    for worker in workers:\n",
    "      worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "      worker.model = copy.deepcopy(self.model)\n",
    "      worker.personalized_model = copy.deepcopy(self.model)\n",
    "      worker.local_model = copy.deepcopy(self.model)\n",
    "\n",
    "  def aggregate_model(self,workers):   \n",
    "    new_params = OrderedDict()\n",
    "    for i,worker in enumerate(workers):\n",
    "      worker_state = worker.model.state_dict()\n",
    "      for key in worker_state.keys():\n",
    "        if i==0:\n",
    "          new_params[key] = worker_state[key]*worker.aggregation_weight\n",
    "        else:\n",
    "          new_params[key] += worker_state[key]*worker.aggregation_weight\n",
    "    self.model.load_state_dict(new_params)\n",
    "    \n",
    "  def send_parameters(self,workers):\n",
    "    nums = 0\n",
    "    for worker in workers:\n",
    "      nums += worker.train_data_num\n",
    "    for worker in workers:\n",
    "        worker.aggregation_weight = 1.0*worker.train_data_num/nums\n",
    "        worker.set_parameters(self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDWEBjgfJYFc"
   },
   "outputs": [],
   "source": [
    "class Worker():\n",
    "  def __init__(self,trainset,valset,testset):\n",
    "    self.trainloader = torch.utils.data.DataLoader(trainset,batch_size=args.batch_size,shuffle=True,num_workers=2)\n",
    "    self.valloader = torch.utils.data.DataLoader(valset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    self.testloader = torch.utils.data.DataLoader(testset,batch_size=args.test_batch,shuffle=False,num_workers=2)\n",
    "    #self.iter_trainloader = iter(self.trainloader)\n",
    "    self.model = eval(model_name)\n",
    "    self.local_model = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.persionalized_model = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.persionalized_model_bar = copy.deepcopy(list(self.model.parameters()))\n",
    "    self.train_data_num = len(trainset)\n",
    "    self.test_data_num = len(testset)\n",
    "    \n",
    "    self.optimizer = pFedMeOptimizer(self.model.parameters(),lr=args.lr,lamda=args.lamda)\n",
    "    \n",
    "  def set_parameters(self, model):\n",
    "    for old_param, new_param, local_param in zip(self.model.parameters(), model.parameters(), self.local_model):\n",
    "        old_param.data = new_param.data.clone()\n",
    "        local_param.data = new_param.data.clone()\n",
    "    #self.local_weight_updated = copy.deepcopy(self.optimizer.param_groups[0]['params'])\n",
    "    \n",
    "  def get_next_train_batch(self):\n",
    "    try:\n",
    "        # Samples a new batch for persionalizing\n",
    "        (X, y) = next(self.iter_trainloader)\n",
    "    except StopIteration:\n",
    "        # restart the generator if the previous generator is exhausted.\n",
    "        self.iter_trainloader = iter(self.trainloader)\n",
    "        (X, y) = next(self.iter_trainloader)\n",
    "    return (X.to(args.device), y.to(args.device))    \n",
    "  '''\n",
    "  def local_train(self):\n",
    "    self.model.train()\n",
    "    optimizer = pFedMeOptimizer(self.model.parameters(),lr=args.lr,lamda=args.lamda)\n",
    "    for epoch in range(args.local_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        data,labels = self.get_next_train_batch()\n",
    "        for i in range(args.K):\n",
    "            self.model = self.model.to(args.device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(data)\n",
    "            loss = args.criterion(outputs,labels)\n",
    "            running_loss += loss.item()\n",
    "            predicted = torch.argmax(outputs,dim=1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            count += len(labels)\n",
    "            loss.backward()\n",
    "            #torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "            self.model = self.model.to('cpu')\n",
    "            personal_model,_ = optimizer.step(self.local_model)\n",
    "        \n",
    "        for new_param,localweight in zip(personal_model,self.local_model.parameters()):\n",
    "            localweight.data = localweight.data - args.lamda * args.lr * (localweight.data - new_param.data)    \n",
    "    \n",
    "    del self.model\n",
    "    \n",
    "    update_parameters(self.personalized_model,personal_model)\n",
    "    return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "  '''\n",
    "\n",
    "  def local_train(self):\n",
    "    if args.dataset_name=='Sent140':\n",
    "        self.model.train()\n",
    "        self.model = self.model.to(args.device)\n",
    "        hidden_train = self.model.init_hidden(args.batch_size)\n",
    "        for epoch in range(args.local_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            count = 0\n",
    "            for (data,labels) in self.trainloader:\n",
    "                self.model.train()\n",
    "                data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "                if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "                    break\n",
    "                data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "                for k in range(args.K):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    hidden_train = repackage_hidden(hidden_train)\n",
    "                    outputs, hidden_train = self.model(data, hidden_train)\n",
    "                    loss = args.criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "                    if k==(args.K-1):\n",
    "                        running_loss += loss.item()\n",
    "                        _, predicted = torch.max(outputs.t(), 1)\n",
    "                        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "                        count += len(labels)\n",
    "                    loss.backward()\n",
    "                    #torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "                    self.persionalized_model_bar,_ = self.optimizer.step(self.local_model)\n",
    "\n",
    "                for new_param, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "                    localweight.data = localweight.data.to(args.device)\n",
    "                    localweight.data = localweight.data - args.lamda* args.lr * (localweight.data - new_param.data)\n",
    "                    localweight.data = localweight.data.to('cpu')\n",
    "\n",
    "        self.update_parameters(self.local_model)\n",
    "\n",
    "        self.model = self.model.to('cpu')\n",
    "        for personal_weight, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "            personal_weight.data = personal_weight.data.to('cpu')\n",
    "            localweight.data = localweight.data.to('cpu')\n",
    "\n",
    "        return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "    \n",
    "    else:\n",
    "        self.model.train() \n",
    "        self.model = self.model.to(args.device)       \n",
    "        for epoch in range(args.local_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            count = 0\n",
    "            for (data,labels) in self.trainloader:\n",
    "                self.model.train()\n",
    "                data,labels = Variable(data),Variable(labels)\n",
    "                data,labels = data.to(args.device),labels.to(args.device)\n",
    "                for k in range(args.K):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    outputs = self.model(data)\n",
    "                    loss = args.criterion(outputs,labels)\n",
    "                    if k==(args.K-1):\n",
    "                        running_loss += loss.item()\n",
    "                        predicted = torch.argmax(outputs,dim=1)\n",
    "                        correct += (predicted==labels).sum().item()\n",
    "                        count += len(labels)\n",
    "                    loss.backward()\n",
    "                    #torch.nn.utils.clip_grad_norm_(self.model.parameters(), args.clip)\n",
    "                    self.persionalized_model_bar,_ = self.optimizer.step(self.local_model)\n",
    "\n",
    "                for new_param, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "                    localweight.data = localweight.data.to(args.device)\n",
    "                    localweight.data = localweight.data - args.lamda* args.lr * (localweight.data - new_param.data)\n",
    "                    localweight.data = localweight.data.to('cpu')\n",
    "\n",
    "        self.update_parameters(self.local_model)\n",
    "\n",
    "        self.model = self.model.to('cpu')\n",
    "        for personal_weight, localweight in zip(self.persionalized_model_bar, self.local_model):\n",
    "            personal_weight.data = personal_weight.data.to('cpu')\n",
    "            localweight.data = localweight.data.to('cpu')\n",
    "\n",
    "        return 100.0*correct/count,running_loss/len(self.trainloader)\n",
    "\n",
    "\n",
    "  def validate(self):\n",
    "    self.model.eval()\n",
    "    self.update_parameters(self.persionalized_model_bar)\n",
    "    self.model = self.model.to(args.device)\n",
    "    acc,loss = test(self.model,args.criterion,self.valloader)\n",
    "    self.model = self.model.to('cpu')\n",
    "    self.update_parameters(self.local_model)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def test(self):\n",
    "    self.model.eval()\n",
    "    self.update_parameters(self.persionalized_model_bar)\n",
    "    self.model = self.model.to(args.device)\n",
    "    acc,loss = test(self.model,args.criterion,self.testloader)\n",
    "    self.model = self.model.to('cpu')\n",
    "    self.update_parameters(self.local_model)\n",
    "    return acc,loss\n",
    "\n",
    "\n",
    "  def update_parameters(self, new_params):\n",
    "    for param , new_param in zip(self.model.parameters(), new_params):\n",
    "      param.data = new_param.data.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7-GY66gROuEU"
   },
   "outputs": [],
   "source": [
    "def train(model,criterion,trainloader,epochs):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.train()\n",
    "      hidden_train = model.init_hidden(args.batch_size)\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "          if args.batch_size != 1 and data.shape[0] != args.batch_size:\n",
    "            break\n",
    "          data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          hidden_train = repackage_hidden(hidden_train)\n",
    "          outputs, hidden_train = model(data, hidden_train) \n",
    "          loss = criterion(outputs.t(), torch.max(labels, 1)[1])\n",
    "          running_loss += loss.item()\n",
    "          _, predicted = torch.max(outputs.t(), 1)\n",
    "          correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)\n",
    "\n",
    "\n",
    "  else:\n",
    "      optimizer = optim.SGD(model.parameters(),lr=args.lr,momentum=args.momentum,weight_decay=args.weight_decay)\n",
    "      model.train()\n",
    "      for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        count = 0\n",
    "        for (data,labels) in trainloader:\n",
    "          data,labels = Variable(data),Variable(labels)\n",
    "          data,labels = data.to(args.device),labels.to(args.device)\n",
    "          optimizer.zero_grad()\n",
    "          outputs = model(data)\n",
    "          loss = criterion(outputs,labels)\n",
    "          running_loss += loss.item()\n",
    "          predicted = torch.argmax(outputs,dim=1)\n",
    "          correct += (predicted==labels).sum().item()\n",
    "          count += len(labels)\n",
    "          loss.backward()\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "          optimizer.step()\n",
    "\n",
    "      return 100.0*correct/count,running_loss/len(trainloader)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oA4URv9mQ3xV"
   },
   "outputs": [],
   "source": [
    "def test(model,criterion,testloader):\n",
    "  if args.dataset_name=='Sent140':\n",
    "      model.eval()\n",
    "      hidden_test = model.init_hidden(args.test_batch)\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data, labels = process_x(data, indd), process_y(labels, indd)\n",
    "        if args.test_batch != 1 and data.shape[0] != args.test_batch:\n",
    "          break\n",
    "        data,labels = torch.from_numpy(data).to(args.device), torch.from_numpy(labels).to(args.device)\n",
    "        hidden_test = repackage_hidden(hidden_test)\n",
    "        outputs, hidden_test = model(data, hidden_test) \n",
    "        running_loss += criterion(outputs.t(), torch.max(labels, 1)[1]).item()\n",
    "        _, predicted = torch.max(outputs.t(), 1)\n",
    "        correct += (predicted == torch.max(labels, 1)[1]).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss\n",
    "\n",
    "\n",
    "  else:\n",
    "      model.eval()\n",
    "      running_loss = 0.0\n",
    "      correct = 0\n",
    "      count = 0\n",
    "      for (data,labels) in testloader:\n",
    "        data,labels = data.to(args.device),labels.to(args.device)\n",
    "        outputs = model(data)\n",
    "        running_loss += criterion(outputs,labels).item()\n",
    "        predicted = torch.argmax(outputs,dim=1)\n",
    "        correct += (predicted==labels).sum().item()\n",
    "        count += len(labels)\n",
    "\n",
    "      accuracy = 100.0*correct/count\n",
    "      loss = running_loss/len(testloader)\n",
    "\n",
    "\n",
    "      return accuracy,loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "id": "-noG_98IR-nZ",
    "outputId": "78a6ebe2-854a-4f83-dc45-5c4ac35b69e8"
   },
   "outputs": [],
   "source": [
    "server = Server()\n",
    "workers = server.create_worker(federated_trainset,federated_valset,federated_testset)\n",
    "acc_train = []\n",
    "loss_train = []\n",
    "acc_valid = []\n",
    "loss_valid = []\n",
    "\n",
    "early_stopping = Early_Stopping(args.partience)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(args.global_epochs):\n",
    "  sample_worker = server.sample_worker(workers)\n",
    "  server.send_parameters(sample_worker)\n",
    "\n",
    "  acc_train_avg = 0.0\n",
    "  loss_train_avg = 0.0\n",
    "  acc_valid_avg = 0.0\n",
    "  loss_valid_avg = 0.0\n",
    "  for worker in sample_worker:\n",
    "    acc_train_tmp,loss_train_tmp = worker.local_train()\n",
    "    acc_valid_tmp,loss_valid_tmp = worker.validate()\n",
    "    acc_train_avg += acc_train_tmp/len(sample_worker)\n",
    "    loss_train_avg += loss_train_tmp/len(sample_worker)\n",
    "    acc_valid_avg += acc_valid_tmp/len(sample_worker)\n",
    "    loss_valid_avg += loss_valid_tmp/len(sample_worker)\n",
    "  server.aggregate_model(sample_worker)\n",
    "  '''\n",
    "  server.model.to(args.device)\n",
    "  for worker in workers:\n",
    "    acc_valid_tmp,loss_valid_tmp = test(server.model,args.criterion,worker.valloader)\n",
    "    acc_valid_avg += acc_valid_tmp/len(workers)\n",
    "    loss_valid_avg += loss_valid_tmp/len(workers)\n",
    "  server.model.to('cpu')\n",
    "  '''\n",
    "  print('Epoch{}  loss:{}  accuracy:{}'.format(epoch+1,loss_valid_avg,acc_valid_avg))\n",
    "  acc_train.append(acc_train_avg)\n",
    "  loss_train.append(loss_train_avg)\n",
    "  acc_valid.append(acc_valid_avg)\n",
    "  loss_valid.append(loss_valid_avg)\n",
    "\n",
    "  if early_stopping.validate(loss_valid_avg):\n",
    "    print('Early Stop')\n",
    "    break\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train time：{}[s]'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_personalized = []\n",
    "loss_test_personalized = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  acc_tmp,loss_tmp = worker.test()\n",
    "  acc_test_personalized.append(acc_tmp)\n",
    "  loss_test_personalized.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_personalized_avg = sum(acc_test_personalized)/len(acc_test_personalized)\n",
    "loss_test_personalized_avg = sum(loss_test_personalized)/len(loss_test_personalized)\n",
    "print('Test(personalized)  loss:{}  accuracy:{}'.format(loss_test_personalized_avg,acc_test_personalized_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_global = []\n",
    "loss_test_global = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  server.model = server.model.to(args.device)\n",
    "  acc_tmp,loss_tmp = test(server.model,args.criterion,worker.testloader)\n",
    "  acc_test_global.append(acc_tmp)\n",
    "  loss_test_global.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "\n",
    "server.model = server.model.to('cpu')\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_test_global_avg = sum(acc_test_global)/len(acc_test_global)\n",
    "loss_test_global_avg = sum(loss_test_global)/len(loss_test_global)\n",
    "print('Test(global)  loss:{}  accuracy:{}'.format(loss_test_global_avg,acc_test_global_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.local_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tune_test_global = []\n",
    "loss_tune_test_global = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i,worker in enumerate(workers):\n",
    "  worker.model = copy.deepcopy(server.model)\n",
    "  worker.model = worker.model.to(args.device)\n",
    "  _,_ = train(worker.model,args.criterion,worker.trainloader,args.local_epochs)\n",
    "  acc_tmp,loss_tmp = test(worker.model,args.criterion,worker.testloader)\n",
    "  acc_tune_test_global.append(acc_tmp)\n",
    "  loss_tune_test_global.append(loss_tmp)\n",
    "  print('Worker{} accuracy:{}  loss:{}'.format(i+1,acc_tmp,loss_tmp))\n",
    "  worker.model = worker.model.to('cpu')\n",
    "  del worker.model\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "acc_tune_test_global_avg = sum(acc_tune_test_global)/len(acc_tune_test_global)\n",
    "loss_tune_test_global_avg = sum(loss_tune_test_global)/len(loss_tune_test_global)\n",
    "print('Test_fine-tune(global)  loss:{}  accuracy:{}'.format(loss_tune_test_global_avg,acc_tune_test_global_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'pFedMe_{}'.format(args.dataset_name)\n",
    "result_path = '../result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = pd.DataFrame(acc_train)\n",
    "loss_train = pd.DataFrame(loss_train)\n",
    "acc_valid = pd.DataFrame(acc_valid)\n",
    "loss_valid = pd.DataFrame(loss_valid)\n",
    "\n",
    "acc_test_global = pd.DataFrame(acc_test_global)\n",
    "loss_test_global = pd.DataFrame(loss_test_global)\n",
    "acc_test_personalized = pd.DataFrame(acc_test_personalized)\n",
    "loss_test_personalized = pd.DataFrame(loss_test_personalized)\n",
    "acc_tune_test_global = pd.DataFrame(acc_tune_test_global)\n",
    "loss_tune_test_global = pd.DataFrame(loss_tune_test_global)\n",
    "\n",
    "\n",
    "acc_train.to_csv(result_path+filename+'_train_acc.csv',index=False, header=False)\n",
    "loss_train.to_csv(result_path+filename+'_train_loss.csv',index=False, header=False)\n",
    "acc_valid.to_csv(result_path+filename+'_valid_acc.csv',index=False, header=False)\n",
    "loss_valid.to_csv(result_path+filename+'_valid_loss.csv',index=False, header=False)\n",
    "acc_test_global.to_csv(result_path+filename+'_test_global_acc.csv',index=False, header=False)\n",
    "loss_test_global.to_csv(result_path+filename+'_test_global_loss.csv',index=False, header=False)\n",
    "acc_test_personalized.to_csv(result_path+filename+'_test_personalized_acc.csv',index=False, header=False)\n",
    "loss_test_personalized.to_csv(result_path+filename+'_test_personalized_loss.csv',index=False, header=False)\n",
    "acc_tune_test_global.to_csv(result_path+filename+'_tune_test_global_acc.csv',index=False, header=False)\n",
    "loss_tune_test_global.to_csv(result_path+filename+'_tune_test_global_loss.csv',index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "FedAvg_femnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
